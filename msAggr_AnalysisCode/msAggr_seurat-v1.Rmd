---
title: "msAggr_seurat-v1"
output: html_notebook
---

Creating new pipeline using seurat v4.0.2 available 2021.06.08

Load libraries required for Seuratv4

```{r setup}
knitr::opts_knit$set(root.dir = "~/Desktop/10XGenomicsData/msAggr_scRNASeq/")
library(dplyr)
library(Seurat)
library(patchwork)
library(ggplot2)
library(clustree)
```
store session info
```{r}
sink("msAggr_seurat-v1.20210608")
sessionInfo()
sink()
```

# A note about using SCTransform versus `ScaleData`
https://bioconductor.org/packages/3.10/workflows/vignettes/simpleSingleCell/inst/doc/batch.html#62_for_gene-based_analyses
>You can also normalize and scale data for the RNA assay. There are numerous resources on this, but Aaron Lun describes why the original log-normalized values should be used for DE and visualizations of expression quite well here:
>
>For gene-based procedures like differential expression (DE) analyses or gene network construction, it is desirable to use the original log-expression values or counts. The corrected values are only used to obtain cell-level results such as clusters or trajectories. Batch effects are handled explicitly using blocking terms or via a meta-analysis across batches. We do not use the corrected values directly in gene-based analyses, for various reasons:
>
>It is usually inappropriate to perform DE analyses on batch-corrected values, due to the failure to model the uncertainty of the correction. This usually results in loss of type I error control, i.e., more false positives than expected.
>
>The correction does not preserve the mean-variance relationship. Applications of common DE methods like edgeR or limma are unlikely to be valid.
>
>Batch correction may (correctly) remove biological differences between batches in the course of mapping all cells onto a common coordinate system. Returning to the uncorrected expression values provides an opportunity for detecting such differences if they are of interest. Conversely, if the batch correction made a mistake, the use of the uncorrected expression values provides an important sanity check.
>
>In addition, the normalized values in SCT and integrated assays don't necessary correspond to per-gene expression values anyway, rather containing residuals (in the case of the scale.data slot for each).



Mess with how to load 4 cell populations into single seurat object



SET SEED?????!!!!!

## Set global variables

```{r}
projectName <- "msAggr"
jackstraw.dim <- 40
```



```{r}
source("msAggr_AnalysisCode/read_10XGenomics_data.R")

```


```{r}
setwd("../cellRanger/") # temporarily changing wd only works if you run the entire chunk at once
data_file.list <- read_10XGenomics_data(sample.list = c("LSKm2", "CMPm2", "MEPm", "GMPm"))
seurat.object.data<-Read10X(data_file.list)
```



```{r}
seurat.object<- CreateSeuratObject(counts = seurat.object.data, min.cells = 3, min.genes = 200, project = projectName)
```

Clean up to free memory

```{r}
remove(seurat.object.data)
```


Add mitochondrial metadata and plot some basic features
```{r}
seurat.object[["percent.mt"]] <- PercentageFeatureSet(seurat.object, pattern = "^mt-")
VlnPlot(seurat.object, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3, pt.size = 0, fill.by = 'orig.ident', )
```


```{r fig.width=4, fig.height=2}
plot1 <- FeatureScatter(seurat.object, feature1 = "nCount_RNA", feature2 = "percent.mt", group.by = "orig.ident", pt.size = 0.01)
plot2 <- FeatureScatter(seurat.object, feature1 = "nCount_RNA", feature2 = "nFeature_RNA", group.by = "orig.ident", pt.size = 0.01)
plot1 + plot2
```


remove low quality cells
require: nFeature_RNA between 200 and 4000 (inclusive)
--require: percent.mt <5--???

```{r}
print(paste("original object:", nrow(seurat.object@meta.data), "cells", sep = " "))
seurat.object <- subset(seurat.object, 
												subset = nFeature_RNA >=200 & 
													nFeature_RNA <= 4000
												)
print(paste("new object:", nrow(seurat.object@meta.data), "cells", sep = " "))
```



## Normalization

Struggling to wrap my head around this one. It seems that SCTransform is best for batch correction, but `NormalizeData` and `ScaleData` are best for DGE. Several vignettes have performed both

`selection.method	
How to choose top variable features. Choose one of :

vst: First, fits a line to the relationship of log(variance) and log(mean) using local polynomial regression (loess). Then standardizes the feature values using the observed mean and expected variance (given by the fitted line). Feature variance is then calculated on the standardized values after clipping to a maximum (see clip.max parameter).

mean.var.plot (mvp): First, uses a function to calculate average expression (mean.function) and dispersion (dispersion.function) for each feature. Next, divides features into num.bin (deafult 20) bins based on their average expression, and calculates z-scores for dispersion within each bin. The purpose of this is to identify variable features while controlling for the strong relationship between variability and average expression.

dispersion (disp): selects the genes with the highest dispersion values`




```{r}
seurat.object <- NormalizeData(seurat.object, normalization.method = "LogNormalize", scale.factor = 10000)
```




```{r}
seurat.object <- FindVariableFeatures(seurat.object, selection.method = "vst", nfeatures = 2000)
```

Find variable features
```{r fig.width = 5, fig.height = 2}
seurat.object <- FindVariableFeatures(seurat.object, selection.method = "vst", nfeatures = 2000)
top10 <- head(VariableFeatures(seurat.object), 10)
plot1 <- VariableFeaturePlot(seurat.object)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
plot1 + plot2

```

Scale data (linear transformation)

```{r}
all.genes <- rownames(seurat.object)
seurat.object <- ScaleData(seurat.object, features = all.genes)
```


### Save progress

```{r}
save.image(file = paste0(projectName, '.RData'))
```


## PCA

linear dimensional reduction. Default are based on VariableFeatures, but can be changed

```{r}
seurat.object <- RunPCA(seurat.object, features = VariableFeatures(object = seurat.object))
```


```{r fig.width=4, fig.height=4}
VizDimLoadings(seurat.object, dims = 1:6, nfeatures = 10, reduction = "pca", ncol = 2)
```

DimPlot colored by orig.ident
```{r}
DimPlot(seurat.object, reduction = "pca", group.by = "orig.ident")
```



Let's put in a concerted effort to pick the right dimensionality using the newest software

```{r}
jackstraw.dim <- 40
seurat.object <- JackStraw(seurat.object, num.replicate = 100, dims = jackstraw.dim) #runs ~50 min
seurat.object <- ScoreJackStraw(seurat.object, dims = 1:jackstraw.dim)
save.image(paste0(projectName, ".RData"))
```


Draw dim.reduction plots

```{r}
JackStrawPlot(seurat.object, dims = 25:36)
```
```{r, figures-side, fig.show='hold', out.width="50%"}
ElbowPlot(seurat.object, ndims = 50)
percent.variance(seurat.object@reductions$pca@stdev)
```
Number of PCs describing X% of variance

```{r}
tot.var <- percent.variance(seurat.object@reductions$pca@stdev, plot.var = FALSE, return.val = TRUE)
paste0("Num pcs for 80% variance:", length(which(cumsum(tot.var) <= 80)))
paste0("Num pcs for 85% variance:", length(which(cumsum(tot.var) <= 85)))
paste0("Num pcs for 90% variance:", length(which(cumsum(tot.var) <= 90)))
paste0("Num pcs for 95% variance:", length(which(cumsum(tot.var) <= 95)))

```

# KNN dimensional reduction
## Dim36

36 PCs describe 95% percent of variance, but one of the earlier critiques was that too many dimensions were included. Best way to deal with this is probably to try a couple of different ones. Let's start with 36 because I dare to rebel.

```{r}
seurat.object <- FindNeighbors(seurat.object, dims = 1:36)
seurat.object <- FindClusters(seurat.object, resolution = 0.5)
```






Generate UMAP data
```{r}
seurat.object <- RunUMAP(seurat.object, dims = 1:36) # took < 1 min
```


Plot the results
```{r}
DimPlot(seurat.object, 
				reduction = "umap"
				) + ggtitle("msAggr dim36 res0.5")

```

```{r}
DimPlot(seurat.object,
				reduction = "umap", 
				group.by = "orig.ident"
				) + ggtitle("msAggt dim36 orig.ident")
```



```{r}
saveRDS(seurat.object, file = "msAggr_dim36.rds")
```


## Dim24
36 dimensions looks very "swoopy". might have asked for too many dimensions. Will try dim = 24, which accounts for 90% of variance

```{r}
# seurat.object <- FindNeighbors(seurat.object, dims = 1:24)
# seurat.object <- FindClusters(seurat.object, resolution = 0.5)
# seurat.object <- RunUMAP(seurat.object, dims = 1:24)
```




Plot the results
```{r}
# DimPlot(seurat.object, 
# 				reduction = "umap"
# 				) + ggtitle("msAggr dim24 res0.5")

```

```{r}
# DimPlot(seurat.object,
# 				reduction = "umap", 
# 				group.by = "orig.ident"
# 				) + ggtitle("msAggt dim24 orig.ident")
```



```{r}
# saveRDS(seurat.object, file = "msAggr_dim24.rds")
```


## KNN conclusion
24 vs 36 dimensions doesn't seem to make that much difference RE overall relationships between cells. Should probbaly create two separate objects and try a few plotting methods, but will start by focusing on 36 dimensions.

# Cluster analysis

Will proceed with dim = 36 and do clustering analysis with a range of clusters. Later can do the tree-based over-clustering assessment

```{r}
seurat.object <- readRDS("msAggr_dim36.rds")
```


## Generate cluster profiles
Picking range of resolutions
```{r}
for(x in c(0.5, 1, 1.5, 2, 2.5)){
	seurat.object <- FindClusters(seurat.object, resolution = x)
}
seurat.object <- RunUMAP(seurat.object, dims = 1:36)
```

Plot the clustering over different resolutions. Going to need a much better color palette, or try mixing in some different symbols

```{r}
for (meta.col in colnames(seurat.object@meta.data)){
	if(grepl(pattern = ("RNA_snn_res"), x = meta.col)==TRUE){
		myplot <- DimPlot(seurat.object, 
											group.by = meta.col,
											reduction = "umap", 
											cols = colorRamps::primary.colors(n = length(levels(seurat.object@meta.data[[meta.col]])))
											) + 
			ggtitle(paste0("msAggr dim36 res", gsub("RNA_snn_res", "", meta.col) ))
		plot(myplot)
	}
}
```




```{r}
# saveRDS(seurat.object, file = "msAggr_dim36.rds")
```

## Evaluate cluster stability
Must ensure we have the right cluster stability, that is, cells that start in the same cluster tend to stay in the same cluster. If your data is over-clustered, cells will bounce between groups.

Following [this tutorial by Matt O.].https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92.

### Clustree
Previously my favourite has been Clustree, which gives a nice visual
NB: For some reason `clustree::clustree()` didn't work, whereas `library(clustree)` followed by `clustree()` did.

```{r fig.height=5}
clustree(seurat.object, prefix = "RNA_snn_res.", node_colour = "sc3_stability") + 
	scale_color_continuous(low = 'red3', high = 'white')
```



```{r fig.height=5}
clustree(seurat.object, prefix = "RNA_snn_res.", expres = 'data', node_colour = "sc3_stability") + 
	scale_color_continuous(low = 'red3', high = 'white')
```


```{r fig.height=5}
clustree(seurat.object, prefix = "RNA_snn_res.", expres = 'scale.data', node_colour = "sc3_stability") + 
	scale_color_continuous(low = 'red3', high = 'white')
```



```{r fig.height=5}
clustree(seurat.object, prefix = "RNA_snn_res.", expres = 'counts', node_colour = "sc3_stability") + 
	scale_color_continuous(low = 'red3', high = 'white')
```




These data suggest that node stability is aweful! Need to figure out if this is a dimensional reduction error or a clustering error.


# Why are clusters so unstable?
Differences could include:
* cells in each population (cellranger v6 includes more cells than cellranger v1, especially in MEP)
* dimensionality is incorrect
* ScaleData didnt account for regression factors (e.g., "nCounts_RNA" or "nFeatures_RNA")
* Incorrect normalization/scaling method
* Clustering is too strict or not strict enough
* neighborhood analysis used wrong parameters
* Should include mitoC filter (there's a chunk of MEP w/ mitoC @ ~40%)
* SCTransform accounts better for sources of variability

```{r}
# Number of filtered cells left in each pop
sapply(c("LSKm2", "CMPm2", "MEPm", "GMPm"), function(x) (c(nrow(seurat.object@meta.data[seurat.object@meta.data$orig.ident == x,]))))
```

```{r}
for (x in c("LSKm2", "CMPm2", "MEPm", "GMPm")){
	h = hist(seurat.object@meta.data[seurat.object@meta.data$orig.ident == x, 'percent.mt'], breaks = 30, plot = FALSE)
	h$density = h$counts/sum(h$counts)*100
	plot(h,freq=FALSE, main =  paste(x, "percent mitoC"), xlab = "percent mitoC", ylab = "Frequency")
}
```

Looks like MEPm is the only sample with that huge MitoC % lump @ 40%. What do these cells look like, otherwise?

```{r fig.width=5}
VlnPlot(subset(seurat.object, subset = orig.ident == "MEPm"), 
				features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 1, pt.size = 0, fill.by = 'ident', flip = TRUE)
```
Save dim36 as is and try clustering analysis @ dim24
```{r}
saveRDS(seurat.object, file = "msAggr_AnalysisCode/msAggr_dim36.rds")
```



# Repeat clustering with dim24
One possibility is that I've included too many dimensions. Will see if 90% increases stability.

```{r}
seurat.object <- FindNeighbors(seurat.object, dims = 1:24)
seurat.object <- FindClusters(seurat.object, resolution = 0.5)
seurat.object <- RunUMAP(seurat.object, dims = 1:24)
```

Save object
```{r}
saveRDS(seurat.object, file = "msAggr_dim24.rds")
```




```{r}
for (meta.col in colnames(seurat.object@meta.data)){
	if(grepl(pattern = ("RNA_snn_res"), x = meta.col)==TRUE){
		myplot <- DimPlot(seurat.object, 
											group.by = meta.col,
											reduction = "umap", 
											cols = colorRamps::primary.colors(n = length(levels(seurat.object@meta.data[[meta.col]])))
											) + 
			ggtitle(paste0("msAggr dim36 res", gsub("RNA_snn_res", "", meta.col) ))
		plot(myplot)
	}
}
```




## Evaluate cluster stability
Must ensure we have the right cluster stability, that is, cells that start in the same cluster tend to stay in the same cluster. If your data is over-clustered, cells will bounce between groups.

Following [this tutorial by Matt O.].https://towardsdatascience.com/10-tips-for-choosing-the-optimal-number-of-clusters-277e93d72d92.

### Clustree
Previously my favourite has been Clustree, which gives a nice visual
NB: For some reason `clustree::clustree()` didn't work, whereas `library(clustree)` followed by `clustree()` did.

```{r fig.height=5}
clustree(seurat.object, prefix = "RNA_snn_res.", node_colour = "sc3_stability") + 
	scale_color_continuous(low = 'red3', high = 'white')
```

Think I'll explore regression factors using SCTransform in new document.
