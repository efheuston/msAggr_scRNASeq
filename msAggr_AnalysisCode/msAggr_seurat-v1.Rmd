---
title: "msAggr_seurat-v1"
output: html_notebook
---

Creating new pipeline using seurat v4.0.2 available 2021.06.08

Load libraries required for Seuratv4

```{r setup}
knitr::opts_knit$set(root.dir = "~/Desktop/10XGenomicsData/msAggr_scRNASeq/")
library(dplyr)
library(Seurat)
library(patchwork)
library(ggplot2)
```
store session info
```{r}
sink("msAggr_seurat-v1.20210608")
sessionInfo()
sink()
```

# A note about using SCTransform versus `ScaleData`
https://bioconductor.org/packages/3.10/workflows/vignettes/simpleSingleCell/inst/doc/batch.html#62_for_gene-based_analyses
>You can also normalize and scale data for the RNA assay. There are numerous resources on this, but Aaron Lun describes why the original log-normalized values should be used for DE and visualizations of expression quite well here:
>
>For gene-based procedures like differential expression (DE) analyses or gene network construction, it is desirable to use the original log-expression values or counts. The corrected values are only used to obtain cell-level results such as clusters or trajectories. Batch effects are handled explicitly using blocking terms or via a meta-analysis across batches. We do not use the corrected values directly in gene-based analyses, for various reasons:
>
>It is usually inappropriate to perform DE analyses on batch-corrected values, due to the failure to model the uncertainty of the correction. This usually results in loss of type I error control, i.e., more false positives than expected.
>
>The correction does not preserve the mean-variance relationship. Applications of common DE methods like edgeR or limma are unlikely to be valid.
>
>Batch correction may (correctly) remove biological differences between batches in the course of mapping all cells onto a common coordinate system. Returning to the uncorrected expression values provides an opportunity for detecting such differences if they are of interest. Conversely, if the batch correction made a mistake, the use of the uncorrected expression values provides an important sanity check.
>
>In addition, the normalized values in SCT and integrated assays don't necessary correspond to per-gene expression values anyway, rather containing residuals (in the case of the scale.data slot for each).



Mess with how to load 4 cell populations into single seurat object



SET SEED?????!!!!!

## Set global variables

```{r}
projectName <- "msAggr"
```



```{r}
source("msAggr_AnalysisCode/read_10XGenomics_data.R")

```


```{r}
setwd("../cellRanger/") # temporarily changing wd only works if you run the entire chunk at once
data_file.list <- read_10XGenomics_data(sample.list = c("LSKm2", "CMPm2", "MEPm", "GMPm"))
seurat.object.data<-Read10X(data_file.list)
```



```{r}
seurat.object<- CreateSeuratObject(counts = seurat.object.data, min.cells = 3, min.genes = 200, project = projectName)
```

Clean up to free memory

```{r}
remove(seurat.object.data)
```


Add mitochondrial metadata and plot some basic features
```{r}
seurat.object[["percent.mt"]] <- PercentageFeatureSet(seurat.object, pattern = "^mt-")
VlnPlot(seurat.object, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3, pt.size = 0, fill.by = 'orig.ident')
```


```{r fig.width=4, fig.height=2}
plot1 <- FeatureScatter(seurat.object, feature1 = "nCount_RNA", feature2 = "percent.mt", group.by = "orig.ident", pt.size = 0.01)
plot2 <- FeatureScatter(seurat.object, feature1 = "nCount_RNA", feature2 = "nFeature_RNA", group.by = "orig.ident", pt.size = 0.01)
plot1 + plot2
```


remove low quality cells
require: nFeature_RNA between 200 and 4000 (inclusive)
--require: percent.mt <5--???

```{r}
print(paste("original object:", nrow(seurat.object@meta.data), "cells", sep = " "))
seurat.object <- subset(seurat.object, 
												subset = nFeature_RNA >=200 & 
													nFeature_RNA <= 4000
												)
print(paste("new object:", nrow(seurat.object@meta.data), "cells", sep = " "))
```



## Normalization

Struggling to wrap my head around this one. It seems that SCTransform is best for batch correction, but `NormalizeData` and `ScaleData` are best for DGE. Several vignettes have performed both

`selection.method	
How to choose top variable features. Choose one of :

vst: First, fits a line to the relationship of log(variance) and log(mean) using local polynomial regression (loess). Then standardizes the feature values using the observed mean and expected variance (given by the fitted line). Feature variance is then calculated on the standardized values after clipping to a maximum (see clip.max parameter).

mean.var.plot (mvp): First, uses a function to calculate average expression (mean.function) and dispersion (dispersion.function) for each feature. Next, divides features into num.bin (deafult 20) bins based on their average expression, and calculates z-scores for dispersion within each bin. The purpose of this is to identify variable features while controlling for the strong relationship between variability and average expression.

dispersion (disp): selects the genes with the highest dispersion values`




```{r}
seurat.object <- NormalizeData(seurat.object, normalization.method = "LogNormalize", scale.factor = 10000)
```




```{r}
seurat.object <- FindVariableFeatures(seurat.object, selection.method = "vst", nfeatures = 2000)
```

Find variable features
```{r fig.width = 5, fig.height = 2}
seurat.object <- FindVariableFeatures(seurat.object, selection.method = "vst", nfeatures = 2000)
top10 <- head(VariableFeatures(seurat.object), 10)
plot1 <- VariableFeaturePlot(seurat.object)
plot2 <- LabelPoints(plot = plot1, points = top10, repel = TRUE)
plot1 + plot2

```

Scale data (linear transformation)

```{r}
all.genes <- rownames(seurat.object)
seurat.object <- ScaleData(seurat.object, features = all.genes)
```


### Save progress

```{r}
save.image(file = paste0(projectName, '.RData'))
```


## PCA

linear dimensional reduction. Default are based on VariableFeatures, but can be changed

```{r}
seurat.object <- RunPCA(seurat.object, features = VariableFeatures(object = seurat.object))
```


```{r fig.width=4, fig.height=4}
VizDimLoadings(seurat.object, dims = 1:6, nfeatures = 10, reduction = "pca", ncol = 2)
```

DimPlot colored by orig.ident
```{r}
DimPlot(seurat.object, reduction = "pca", group.by = "orig.ident")
```



Let's put in a concerted effort to pick the right dimensionality using the newest software

```{r}
seurat.object <- JackStraw(seurat.object, num.replicate = 100, dims = 40) #runs ~50 min
seurat.object <- ScoreJackStraw(seurat.object, dims = 1:40)
save.image(paste0(projectName, ".RData"))
```


Draw dim.reduction plots

```{r}
# plot1 <- JackStrawPlot(seurat.object, dims = 1:50)
```
```{r, figures-side, fig.show='hold', out.width="50%"}
ElbowPlot(seurat.object, ndims = 50)
percent.variance(seurat.object@reductions$pca@stdev)

```
Number of PCs describing X% of variance

```{r}
tot.var <- percent.variance(seurat.object@reductions$pca@stdev, plot.var = FALSE, return.val = TRUE)
paste0("Num pcs for 80% variance:", length(which(cumsum(tot.var) <= 80)))
paste0("Num pcs for 85% variance:", length(which(cumsum(tot.var) <= 85)))
paste0("Num pcs for 90% variance:", length(which(cumsum(tot.var) <= 90)))
paste0("Num pcs for 95% variance:", length(which(cumsum(tot.var) <= 95)))

```




36 PCs describe 95% percent of variance, but one of the earlier critiques was that too many dimensions were included. Best way to deal with this is probably to try a couple of different ones. Let's start with 36 because I dare to rebel.

```{r}
seurat.object <- FindNeighbors(seurat.object, dims = 1:36)
seurat.object <- FindClusters(seurat.object, resolution = 0.5)
```






Generate UMAP data
```{r}
seurat.object <- RunUMAP(seurat.object, dims = 1:36) # took < 1 min
```


Plot the results
```{r}
DimPlot(seurat.object, 
				reduction = "umap"
				) + ggtitle("msAggr dim36 res0.5")

```

```{r}
DimPlot(seurat.object,
				reduction = "umap", 
				group.by = "orig.ident"
				) + ggtitle("msAggt dim36 orig.ident")
```



```{r}
saveRDS(seurat.object, file = "msAggr_dim36.rds")
```

Not a fan of how "swoopy" it looks. Could be to many dimensions? May as well try 90%

```{r}
seurat.object <- FindNeighbors(seurat.object, dims = 1:24)
seurat.object <- FindClusters(seurat.object, resolution = 0.5)
seurat.object <- RunUMAP(seurat.object, dims = 1:24)
```




Plot the results
```{r}
DimPlot(seurat.object, 
				reduction = "umap"
				) + ggtitle("msAggr dim24 res0.5")

```

```{r}
DimPlot(seurat.object,
				reduction = "umap", 
				group.by = "orig.ident"
				) + ggtitle("msAggt dim24 orig.ident")
```



```{r}
saveRDS(seurat.object, file = "msAggr_dim24.rds")
```



24 vs 36 dimensions doesn't seem to make that much difference RE overall relationships. Should probbaly create two separate objects and try a few plotting methods.
